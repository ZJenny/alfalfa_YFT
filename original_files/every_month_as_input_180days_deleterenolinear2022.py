# -*- coding: utf-8 -*-
"""every_month_as_input_180days-deleterenolinear2022.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Tuug-3znrhULp7CG5KZQuagvJtjQ_LA
"""

import os
# Set the number of threads for BLAS and OpenMP
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['VECLIB_MAXIMUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'

import pandas as pd
import numpy as np
new_data = pd.read_csv('Merge_Allcombine_Data_With_Location_Year.csv')
# Drop specified columns: 'Growth stage' and 'Leaf Area Index (LAI)'
columns_to_drop = ['Growth stage', 'Leaf Area Index (LAI)']
data_after_dropping = new_data.drop(columns=columns_to_drop)

# Check the data types of each column to identify non-numeric columns
non_numeric_columns = new_data.select_dtypes(exclude=['number']).columns

def calculate_rolling_features_with_logic(data, dates, window_size=180):
    features_list = []

    # generate time-window description
    window_label = f"({window_size}d)"

    # Split data into BushlandLysimeter and other experiment groups based on location
    bushland_data = data[data['location'].str.contains('BushlandLysimeters', case=False, na=False)]
    other_data = data[~data['location'].str.contains('BushlandLysimeters', case=False, na=False)]

    # Process BushlandLysimeter data
    for date in dates:
        end_date = date
        start_date = date - pd.Timedelta(days=window_size)
        year = end_date.year  # Extract year from end_date

        mask = (bushland_data['Timestamp'] >= start_date) & (bushland_data['Timestamp'] <= end_date)
        window_data = bushland_data.loc[mask]
        weather_data = window_data.drop_duplicates(subset=['Timestamp'])  # Define weather_data

        for plot in window_data['Plot'].unique():
            plot_data = window_data[window_data['Plot'] == plot]

            # Calculate statistical features for BushlandLysimeter
            def calculate_stats(column):
                return {
                    'min': column.min(),
                    'max': column.max(),
                    'mean': column.mean(),
                    'sum': column.sum()
                }

            stats = {
                'Total Precipitation (mm)': calculate_stats(plot_data['Total Precipitation (mm)']),
                'Average Air Temperature (Deg C)': calculate_stats(plot_data['Average Air Temperature (Deg C)']),
                'Total Solar Radiation (kW-hr/m2)': calculate_stats(plot_data['Total Solar Radiation (kW-hr/m2)']),
                'Average Wind Speed (m/s)': calculate_stats(plot_data['Average Wind Speed (m/s)']),
                'Relative Humidity (%)': calculate_stats(plot_data['Relative Humidity (%)']),
                'Irrig. amount (in)': calculate_stats(plot_data['Irrig. amount (in)']),
                'Ave. SWD (%)': calculate_stats(plot_data['Ave. SWD (%)']),
                'Plant height (cm)': calculate_stats(plot_data['Plant height (cm)'])
            }

            features_list.append({
                'Timestamp': end_date,
                'Plot': plot,
                'experiment_info': f'BushlandLysimeters_{year}',
                **{f"{feature} {window_label} {stat}": value
                   for feature, stats_dict in stats.items()
                   for stat, value in stats_dict.items()}
            })

    # Process other experiment_info groups
    for experiment in other_data['experiment_info'].unique():
        experiment_group = other_data[other_data['experiment_info'] == experiment]

        for date in dates:
            end_date = date
            start_date = date - pd.Timedelta(days=window_size)

            mask = (experiment_group['Timestamp'] >= start_date) & (experiment_group['Timestamp'] <= end_date)
            window_data = experiment_group.loc[mask]

            # Weather features calculation (shared across all plots in the same experiment)
            weather_data = window_data.drop_duplicates(subset=['Timestamp'])
            def calculate_stats(column):
                return {
                    'min': column.min(),
                    'max': column.max(),
                    'mean': column.mean(),
                    'sum': column.sum()
                }

            stats = {
                'Total Precipitation (mm)': calculate_stats(weather_data['Total Precipitation (mm)']),
                'Average Air Temperature (Deg C)': calculate_stats(weather_data['Average Air Temperature (Deg C)']),
                'Total Solar Radiation (kW-hr/m2)': calculate_stats(weather_data['Total Solar Radiation (kW-hr/m2)']),
                'Average Wind Speed (m/s)': calculate_stats(weather_data['Average Wind Speed (m/s)']),
                'Relative Humidity (%)': calculate_stats(weather_data['Relative Humidity (%)'])
            }

            for plot in window_data['Plot'].unique():
                plot_data = window_data[window_data['Plot'] == plot]

                # Calculate plot-specific features
                plot_stats = {
                    'Irrig. amount (in)': calculate_stats(plot_data['Irrig. amount (in)']),
                    'Ave. SWD (%)': calculate_stats(plot_data['Ave. SWD (%)']),
                    'Plant height (cm)': calculate_stats(plot_data['Plant height (cm)'])
                }

                features_list.append({
                    'Timestamp': end_date,
                    'Plot': plot,
                    'experiment_info': experiment,
                    **{f"{feature} {window_label} {stat}": value
                       for feature, stats_dict in {**stats, **plot_stats}.items()
                       for stat, value in stats_dict.items()}
                })

    # Convert list of dicts to DataFrame
    features_df = pd.DataFrame(features_list)
    return features_df

# Ensure 'Timestamp' is in datetime format
new_data['Timestamp'] = pd.to_datetime(new_data['Timestamp'])

# Identify harvest dates from new_data
harvest_dates = new_data[new_data['Dry yield (ton/ha)'].notna()]['Timestamp'].unique()
harvest_dates = pd.to_datetime(harvest_dates)

# Ensure 'Timestamp' is in datetime format
new_data['Timestamp'] = pd.to_datetime(new_data['Timestamp'])
# Encode Day of Year as sin and cos
new_data['sin_day'] = np.sin(2 * np.pi * new_data['Day of Year'] / 365) #new add:1.12
new_data['cos_day'] = np.cos(2 * np.pi * new_data['Day of Year'] / 365) #new add:1.12

# Calculate rolling features for new_data with 180-day window
rolling_features = calculate_rolling_features_with_logic(new_data, harvest_dates, window_size=180)

# Merge rolling features with yield and crop characteristics
merged_data = pd.merge(
    rolling_features,
    new_data[['Timestamp', 'Plot', 'Dry yield (ton/ha)', 'Fall Dormancy*', 'Winterhardiness**', 'Day of Year', 'Alfalfa variety', 'experiment_info','sin_day','cos_day']],
    on=['Timestamp', 'Plot', 'experiment_info'],
    how='left'
)

# Drop rows where target variable is missing
merged_data = merged_data.dropna(subset=['Dry yield (ton/ha)'])

# Display the first few rows of the merged data
merged_data.head()

merged_data.to_csv('check.csv',index=False)

def calculate_monthly_features_by_experiment(data):
    """
    Calculates monthly (Jan~May) weather and plot-level features in "wide" format:

    - BushlandLysimeters:
      *All* features (weather + plot) are computed per Plot.
      -> Each row in the final DataFrame corresponds to (experiment_info, Plot),
         and has columns for M1..M5 stats.

    - Non-BushlandLysimeters:
      Weather features are computed at the *entire experiment_info* level (shared by all plots).
      Plot-level features (irrigation, SWD, plant height, etc.) are computed per Plot.
      -> Same one-row-per-Plot output, but the weather stats are identical for all Plots
         in the same experiment_info.
    """

    # 1) Ensure Timestamp is datetime
    data['Timestamp'] = pd.to_datetime(data['Timestamp'], errors='coerce')
    data = data.dropna(subset=['Timestamp'])

    # 2) Keep only Jan~May
    data = data[data['Timestamp'].dt.month.isin([1, 2, 3, 4, 5])]

    # 3) Split into Bushland and non-Bushland
    bushland_data = data[data['location'].str.contains('BushlandLysimeters', case=False, na=False)]
    other_data = data[~data['location'].str.contains('BushlandLysimeters', case=False, na=False)]

    # Specify which columns are considered "weather" vs. "plot-specific"
    weather_cols = [
        'Total Precipitation (mm)',
        'Average Air Temperature (Deg C)',
        'Total Solar Radiation (kW-hr/m2)',
        'Average Wind Speed (m/s)',
        'Relative Humidity (%)'
    ]
    plot_cols = [
        'Irrig. amount (in)',
        'Ave. SWD (%)',
        'Plant height (cm)'
    ]

    # Helper: compute min, max, mean, sum for a single column's data
    def calc_stats(series, month):
        return {
            f"{series.name} (M{month}) min":  series.min(),
            f"{series.name} (M{month}) max":  series.max(),
            f"{series.name} (M{month}) mean": series.mean(),
            f"{series.name} (M{month}) sum":  series.sum()
        }

    features_list = []  # Will accumulate final row dicts

    # --------------------------------------------------------------------------
    # A) BushlandLysimeters:
    #    Weather + plot features both computed at the Plot level
    # --------------------------------------------------------------------------
    for experiment in bushland_data['experiment_info'].unique():
        exp_group = bushland_data[bushland_data['experiment_info'] == experiment]

        # Each row in final result => (experiment_info, Plot)
        for plot_id in exp_group['Plot'].unique():
            plot_data = exp_group[exp_group['Plot'] == plot_id]

            # We'll build one row with M1..M5 stats for this plot
            row_dict = {
                'experiment_info': experiment,
                'Plot': plot_id
            }

            # Loop months 1..5 and gather stats in "wide" columns
            for month in [1, 2, 3, 4, 5]:
                month_data = plot_data[plot_data['Timestamp'].dt.month == month]

                # (1) Weather columns at plot-level
                for wcol in weather_cols:
                    if wcol in month_data.columns:
                        row_dict.update(calc_stats(month_data[wcol], month))

                # (2) Plot-specific columns
                for pcol in plot_cols:
                    if pcol in month_data.columns:
                        row_dict.update(calc_stats(month_data[pcol], month))

            # After collecting all 5 months into row_dict, append once
            features_list.append(row_dict)

    # --------------------------------------------------------------------------------
    # B) Non-BushlandLysimeters:
    #    - Weather features = experiment_info level (shared by all plots)
    #    - Plot features = per Plot
    # --------------------------------------------------------------------------------
    for experiment in other_data['experiment_info'].unique():
        exp_group = other_data[other_data['experiment_info'] == experiment]

        # First, compute "group-level" weather stats for all 5 months
        # We'll store them in a dict keyed by month.
        group_weather_stats = {m: {} for m in [1,2,3,4,5]}
        for month in [1, 2, 3, 4, 5]:
            month_data_whole = exp_group[exp_group['Timestamp'].dt.month == month]
            for wcol in weather_cols:
                if wcol in month_data_whole.columns:
                    group_weather_stats[month].update(calc_stats(month_data_whole[wcol], month))

        # Now, for each Plot, compute its plot-specific stats (M1..M5)
        # and combine with the group-level weather stats
        for plot_id in exp_group['Plot'].unique():
            plot_data = exp_group[exp_group['Plot'] == plot_id]

            row_dict = {
                'experiment_info': experiment,
                'Plot': plot_id
            }

            for month in [1, 2, 3, 4, 5]:
                # Add the group's weather stats for this month
                row_dict.update(group_weather_stats[month])

                # Add this plot's stats for this month
                month_data_plot = plot_data[plot_data['Timestamp'].dt.month == month]
                for pcol in plot_cols:
                    if pcol in month_data_plot.columns:
                        row_dict.update(calc_stats(month_data_plot[pcol], month))

            # One row for this Plot
            features_list.append(row_dict)

    # 6) Convert list of dicts to a DataFrame (wide format)
    monthly_features_df = pd.DataFrame(features_list)

    # 7) Optional: reorder columns. For instance, keep 'experiment_info' and 'Plot' at front.
    core_cols = ['experiment_info', 'Plot']
    other_cols = [c for c in monthly_features_df.columns if c not in core_cols]
    monthly_features_df = monthly_features_df[core_cols + other_cols]

    return monthly_features_df

# Calculate monthly features by experiment for January to May
monthly_features = calculate_monthly_features_by_experiment(new_data)
monthly_features.head(30)

merged_data = pd.merge(
    merged_data,
    monthly_features,
    on=['experiment_info', 'Plot'],
    how='left',
    suffixes=('', '_monthly'))

#merged_data.to_csv('check.csv',index=False)

## 这一步待删除，只是查看哪些列是完全missing的
# Identify columns that are completely missing in the dataset
# Load the data
file_path = 'check.csv'
data = pd.read_csv(file_path)
data = data[data['Dry yield (ton/ha)'] >= 0].reset_index(drop=True)  #wait for nevada response
## to check which column with full missing values and drop it
fully_missing_columns = data.columns[data.isna().all()]

# Display these columns
fully_missing_columns.tolist()

import pandas as pd
from sklearn.impute import KNNImputer

# Load the data
file_path = 'check.csv'
data = pd.read_csv(file_path)
data = data[data['Dry yield (ton/ha)'] >= 0].reset_index(drop=True)

# Identify columns that are completely missing and drop them
fully_missing_columns = data.columns[data.isna().all()]
data = data.drop(columns=fully_missing_columns)

# Find columns containing "Irrig. amount"
irrig_columns = [col for col in data.columns if 'Irrig. amount' in col]

# Fill missing values in "Irrig. amount" columns with 0
data[irrig_columns] = data[irrig_columns].fillna(0)

# Identify numeric columns for KNN imputation excluding "Irrig. amount" columns
numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns
knn_columns = [col for col in numeric_columns if col not in irrig_columns]

# Perform KNN imputation for remaining numeric columns
imputer = KNNImputer(n_neighbors=5)
data[knn_columns] = imputer.fit_transform(data[knn_columns])

# Define train and test experiments
train_experiments = [
    "BushlandCenterPivot_2022", "BushlandLysimeters_1996", "BushlandLysimeters_1997",
    "BushlandLysimeters_1998", "Fallon_1973_Final", "Fallon_1974_Final", "Fallon_1975_Final",
    "Fallon_1976_Final", "Fallon_1977_Final", "Fallon_1981_Final", "RenoDripIrrigation_2021",
    "RenoDripIrrigation_2022", "RenoLinearMoveIrrigation_2023", "Fallon_1978_Final"
]

test_experiments = [
    "BushlandCenterPivot_2023", "BushlandLysimeters_1999", "Fallon_1982",
    "RenoDripIrrigation_2023"
]

# Assign rows to train/test based on updated experiment groups
train_data = data[data['experiment_info'].isin(train_experiments)].reset_index(drop=True)
test_data = data[data['experiment_info'].isin(test_experiments)].reset_index(drop=True)

# Calculate sizes of train and test datasets
train_data_size = train_data.shape[0]
test_data_size = test_data.shape[0]

train_data_size, test_data_size

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder
import numpy as np
import matplotlib.pyplot as plt
# calculate feature importance and choose top15, 因为特征间存在较强的交互关系（如冗余特征或复杂非线性关系），所以使用 XGBoost。
def select_top_features(X, y, n_top_features=15):
    # Train a preliminary XGBoost model to calculate feature importance
    temp_model = XGBRegressor(random_state=42, objective='reg:squarederror')
    temp_model.fit(X, y)

    # Get feature importance scores
    importance = temp_model.feature_importances_
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': importance
    }).sort_values(by='Importance', ascending=False)

    # Print all features and their importance
    print("Feature Importance (sorted):")
    print(feature_importance)

    # Select top n features
    top_features = feature_importance.head(n_top_features)['Feature'].values
    return top_features, feature_importance

# Impute missing values in train and test data
def preprocess_train_data_with_imputation(df):
    df = df.copy()
    df = df.drop(columns=['Timestamp', 'Plot', 'experiment_info'])  # Drop non-essential columns
    label_encoder = LabelEncoder()
    df['Alfalfa variety'] = label_encoder.fit_transform(df['Alfalfa variety'])

    # Separate features and target
    X = df.drop(columns=['Dry yield (ton/ha)'])
    y = df['Dry yield (ton/ha)']

    # Impute missing values in features
    imputer = SimpleImputer(strategy='mean')
    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

    return X, y

def preprocess_test_data_with_imputation(df):
    df = df.copy()
    df = df.drop(columns=['Timestamp', 'Plot', 'experiment_info', 'Dry yield (ton/ha)'])
    label_encoder = LabelEncoder()
    df['Alfalfa variety'] = label_encoder.fit_transform(df['Alfalfa variety'])

    # Impute missing values in features
    imputer = SimpleImputer(strategy='mean')
    X = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

    return X

# Apply preprocessing with imputation
X_train, y_train = preprocess_train_data_with_imputation(train_data)
X_test = preprocess_test_data_with_imputation(test_data)
y_test_actual = test_data['Dry yield (ton/ha)']

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Setup GridSearchCV for XGBoost Regressor
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7,9],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Initialize XGBoost model
xgb_model = XGBRegressor(random_state=42, objective='reg:squarederror')

# Perform cross-validation with GridSearchCV
grid_search_xgb = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid_xgb,
    cv=7,  # 5-fold cross-validation
    n_jobs=-1,
    scoring='neg_mean_squared_error',
    verbose=1
)

grid_search_xgb.fit(X_train, y_train)

# Get the best model from cross-validation
best_params_xgb = grid_search_xgb.best_params_
best_model_xgb = grid_search_xgb.best_estimator_

# Evaluate the best model on test data
y_test_pred_xgb = best_model_xgb.predict(X_test)

# Calculate performance metrics on test set
test_rmse_xgb = mean_squared_error(y_test_actual, y_test_pred_xgb, squared=False)
test_r2_xgb = r2_score(y_test_actual, y_test_pred_xgb)

# Perform cross-validation on training data for the final model
cv_scores = cross_val_score(best_model_xgb, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
cv_rmse_scores = np.sqrt(-cv_scores)

# Print results
print(f"XGBoost Cross-Validation RMSE Scores: {cv_rmse_scores}")
print(f"XGBoost Cross-Validation Mean RMSE: {cv_rmse_scores.mean()}")
print(f"XGBoost Test RMSE: {test_rmse_xgb}")
print(f"XGBoost Test R^2: {test_r2_xgb}")
print(f"Best Parameters (XGBoost): {best_params_xgb}")

# Plot predictions vs actual values
plt.figure(figsize=(14, 7))
plt.plot(y_test_actual.values, label='Actual Yield', color='blue', linestyle='-', marker='o')
plt.plot(y_test_pred_xgb, label='Predicted Yield (XGBoost)', color='purple', linestyle='-', marker='x')
plt.xlabel('Sample Index')
plt.ylabel('Dry Yield (ton/ha)')
plt.title('Actual vs Predicted Dry Yield on Test Set (XGBoost)')
plt.legend()
plt.show()

